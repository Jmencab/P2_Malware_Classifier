################################################
#              Feature Engineering             #
################################################
from pprint import pprint
from collections import Counter
from tqdm import tqdm
import os
import util
import pandas as pd
import numpy as np
from sklearn.naive_bayes import BernoulliNB as bnb
from sklearn.naive_bayes import MultinomialNB as mnb
try:
    import xml.etree.cElementTree as ET
except ImportError:
    import xml.etree.ElementTree as ET

'''
Uses extract_features as blueprint for extracting list of trees
Also returns list of classes and ids
'''
def extract_tree(direc="train"):
    classes = []
    ids = []
    tree_list = []
    for datafile in tqdm(os.listdir(direc)):
        # extract id and true class (if available) from filename
        id_str,clazz = datafile.split('.')[:2]
        ids.append(id_str)
        # add target class if this is training data
        try:
            classes.append(util.malware_classes.index(clazz))
        except ValueError:
            # we should only fail to find the label in our list of malware classes
            # if this is test data, which always has an "X" label
            assert clazz == "X"
            classes.append(-1)
        tree_list.append(ET.parse(os.path.join(direc,datafile)))

    return tree_list, classes, ids

def system_call_count_feats(tree):

	c = Counter()
	in_all_section = False
	for el in tqdm(tree.iter()):
	    # ignore everything outside the "all_section" element
	    if el.tag == "all_section" and not in_all_section:
	        in_all_section = True
	    elif el.tag == "all_section" and in_all_section:
	        in_all_section = False
	    elif in_all_section:
	        c['num_system_calls'] += 1
	return c

def perSysCallCount(tree, callSet):
    """
    arguments:
      tree is an xml.etree.ElementTree object
    returns:
        a dictionary of the system calls seen across all trees
    """
    callDict = dict()
    in_all_section = False
    for el in tqdm(tree.iter()):
        # ignore everything outside the "all_section" element
        if el.tag == "all_section" and not in_all_section:
            in_all_section = True
        elif el.tag == "all_section" and in_all_section:
            in_all_section = False
        elif in_all_section:
            if el.tag in callDict:
                callDict[el.tag] += 1
            else:
                callDict[el.tag] = 1
                if el.tag not in callSet:
                    callSet.add(el.tag)

    return callDict

def newPerSysCallCountFile(dict_list, classes, ids, writefile, sysCallSet):
    f = open(writefile, "w+")
    headers = 'Id,Class,'
    columns = ['Id', 'Class']
    for call in sysCallSet:
        headers+= call + ','
        columns.append(call)
    headers = headers.rstrip(',') +"\n"
    f.write(headers)
    for index, dictTree in enumerate(tqdm(dict_list)):
        line = ids[index] + ',' + str(classes[index]) + ','
        countList = ['0'] * (len(sysCallSet) + 2)
        for key,val in dictTree.items():
            countList[columns.index(key)] = str(val)
        for count in countList[2:]:
            line += count + ','
        line = line.rstrip(',') + "\n"
        f.write(line)
    f.close()

    # columns = ['Id', 'Class']
    # for call in sysCallSet:
    #     columns.append(call)
    # df = pd.DataFrame(columns=columns)
    # for index, dictTree in enumerate(tqdm(dict_list)):
    #     countArr = np.empty(len(columns), dtype='S41')
    #     countArr[0] = str(ids[index])
    #     countArr[1] = str(classes[index])
    #     for key,val in dictTree.items():
    #         countArr[columns.index(key)] = str(val)
    #     dfArr = pd.DataFrame(countArr)
    #     df = pd.concat((df,dfArr), axis =1)
    # df.to_csv(writefile)


def newCountFile(tree_list, classes, ids, writefile):
    f = open(writefile, "w+")
    f.write("Id,Class,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,f13,f14,f15,f16,f17,f18,f19\n")
    index = 0
    for tree in tree_list:
        count = bin(system_call_count_feats(tree)['num_system_calls']).lstrip('0b')
        binStr = ''
        pad_zeros = 20-len(count)
        for _ in range(pad_zeros):
            binStr += "0,"
        for num in count:
            binStr += (num + ',')
        binStr = binStr.rstrip(',')
        f.write(ids[index] + "," + str(classes[index]) + "," + binStr + "\n")
        index +=1
    f.close()

def main():
    priors = [.0369,.0162,.012,.0103,.0133,.0126,.0172,.0133,.5214,.0068,.1756,.0104,.1218,.0191,.013]

    ##########################
    ####System Counts#########
    ##########################
    # define global set for creating data frames
    # test_tree_list, test_classes, test_ids = extract_tree("test")
    # globalSetTest = set()
    # dictListTest = list()
    # for tree in test_tree_list:
    #     dictListTest.append(perSysCallCount(tree, globalSetTest))

    # train_tree_list, train_classes, train_ids = extract_tree("train")
    # dictListTrain = list()
    # for tree in train_tree_list:
    #     dictListTrain.append(perSysCallCount(tree, globalSetTest))

    # newPerSysCallCountFile(dictListTest,test_classes, test_ids, "perSysCountsTest.csv", globalSetTest)
    # newCountFile(test_tree_list, test_classes, test_ids, "choppyTest.csv")
    # del test_tree_list,test_classes,dictListTest,test_ids

    # newPerSysCallCountFile(dictListTrain,train_classes,train_ids, "perSysCountsTrain.csv",globalSetTest)
    #newCountFile(train_tree_list, train_classes, train_ids, "choppyTrain.csv")
    # del train_tree_list,train_classes,train_ids,dictListTrain

    ###############################################
    #######Per-Tree, Per-System Call Counts########
    ###############################################
    """
    Read in train and test as Pandas DataFrames
    """
    # df_train = pd.read_csv("choppyTrain.csv")
    # df_test = pd.read_csv("choppyTest.csv")
    df_train = pd.read_csv("perSysCountsTrain.csv")
    df_test = pd.read_csv("perSysCountsTest.csv")
    #store class values
    Y_train = df_train.Class.values
    testID = df_test.Id.values
    #row where testing examples start
    test_idx = df_train.shape[0]
    df_all = pd.concat((df_train, df_test), axis=0)
    del df_train
    del df_test
    df_all = df_all.drop(['Id'], axis=1)
    df_all = df_all.drop(['Class'], axis=1)
    vals = df_all.values
    del df_all
    X_train = vals[:test_idx]
    X_test = vals[test_idx:]
    del vals
    # clf = bnb(class_prior=priors)
    # clf.fit(X_train, Y_train)
    clf = mnb(class_prior=priors)
    clf.fit(X_train,Y_train)
    del X_train
    del Y_train
    # bnb_predict = clf.predict(X_test)
    mnb_predict = clf.predict(X_test)
    # util.write_predictions(bnb_predict,test_ids,"ChoppySingleBNB.csv")
    util.write_predictions(mnb_predict,testID,"PerSysCallCountsBNB.csv")

if __name__ == "__main__":
    main()
###########################################################################
from tqdm import tqdm
import os
import operator as op
import csv
from csvFactory import DIR
try:
    import xml.etree.cElementTree as ET
except ImportError:
    import xml.etree.ElementTree as ET

features = []

def file_grams (n, datafile, direc, tags=True) :
  # convert file to tree
  tree = ET.parse(os.path.join(direc, datafile))
  # convert tree into tag list or byte list
  if tags == True:
    elList = []
    for el in tree.iter():
      elList.append(el.tag)
  else:
    bytestr = ET.tostring(tree.getroot(), encoding="UTF-8")
    elList = list(bytestr)
  # create list of all n_grams
  gram_list = zip(*[elList[i:] for i in range(n)])
  return gram_list

def find_feats (n, nbst, direc=DIR):
  gram_dict = dict()
  # add all grams from all files
  for datafile in tqdm(os.listdir(direc)):
    gram_list = file_grams (n, datafile, DIR)
    # create dictionary with n_grams and their values
    for el in gram_list:
      if el in gram_dict:
        gram_dict[el] += 1
      else:
        gram_dict[el] = 1
  # pick most frequent keys as features
  all_feats = sorted(gram_dict, key=gram_dict.get, reverse=True)
  return all_feats[:nbst]

def file_vals (n, nbst, datafile, direc) :
  global features
  if len(features) < n:
    features.append(find_feats (n, nbst))
  gram_list = file_grams (n, datafile, direc)
  vals = []
  for feat in features[n-1]:
    vals.append(gram_list.count(feat))
  return vals
#######################################################################
import os
import ngram

# function to get byte size per file
def byte_size_vals (datafile, direc, size):
  return [os.path.getsize(direc + "/" + datafile)]

# function to get 1-gram
def gram1_vals (datafile, direc, BST1):
  return ngram.file_vals(1, BST1, datafile, direc, 'tags')

# function to get 2-gram
def gram2_vals (datafile, direc, BST2):
  return ngram.file_vals(2, BST2, datafile, direc, 'tags')

# function to get 3-gram
def gram3_vals (datafile, direc, BST3):
  return ngram.file_vals(3, BST3, datafile, direc, 'tags')

# function to get 4-gram
def gram4_vals (datafile, direc, BST4):
  return ngram.file_vals(4, BST4, datafile, direc, 'tags')

# function to get 1-gram
def gram1_vals_b (datafile, direc, BST1):
  return ngram.file_vals(1, BST1, datafile, direc, 'bytes')

# function to get 2-gram
def gram2_vals_b (datafile, direc, BST2):
  return ngram.file_vals(2, BST2, datafile, direc, 'bytes')

# function to get 3-gram
def gram3_vals_b (datafile, direc, BST3):
  return ngram.file_vals(3, BST3, datafile, direc, 'bytes')

# function to get 4-gram
def gram4_vals_b (datafile, direc, BST4):
  return ngram.file_vals(4, BST4, datafile, direc, 'bytes')

# function to get 1-gram
def gram1_vals_c (datafile, direc, BST1):
  return ngram.file_vals(1, BST1, datafile, direc, 'chars')

# function to get 2-gram
def gram2_vals_c (datafile, direc, BST2):
  return ngram.file_vals(2, BST2, datafile, direc, 'chars')

# function to get 3-gram
def gram3_vals_c (datafile, direc, BST3):
  return ngram.file_vals(3, BST3, datafile, direc, 'chars')

# function to get 4-gram
def gram4_vals_c (datafile, direc, BST4):
  return ngram.file_vals(4, BST4, datafile, direc, 'chars')
##############################################################
import features
from tqdm import tqdm
import os
import csv
import util

# writes csv files for feature functions given a test and train directory
# needs files features.py and ngram.py

# this is the training directory and test directory
DIR = "train"
DIRR = "test"

# number of features to be extracted for each n-gram
# you can use any numbers as long as they are DISTINCT from each other
BST1, BST2, BST3, BST4 = 64, 128, 256, 512

# don't change this unless you're adding feature functions
NUMFUN = 5

def write_to_csv ():

	writer = []
	sizes = [["Size"], list(range(BST1)), list(range(BST2)), list(range(BST3)), list(range(BST4)) ]
	funs = [features.byte_size_vals, features.gram1_vals, features.gram2_vals,
					features.gram3_vals, features.gram4_vals]

	# open all files to write in
	with open ('bytesizetrain.csv', 'w') as s, \
	     open ('gram1train.csv', 'w') as g1, \
	     open ('gram2train.csv', 'w') as g2, \
	     open ('gram3train.csv', 'w') as g3, \
	     open ('gram4train.csv', 'w') as g4, \
	     open ('bytesizetest.csv', 'w') as sr, \
	     open ('gram1test.csv', 'w') as g1r, \
	     open ('gram2test.csv', 'w') as g2r, \
	     open ('gram3test.csv', 'w') as g3r, \
	     open ('gram4test.csv', 'w') as g4r:

	  # initialize a writer for each file
		for i in [s, g1, g2, g3, g4, sr, g1r, g2r, g3r, g4r]:
			writer.append(csv.writer(i, lineterminator='\n'))

		# write column headers
		for i in range(NUMFUN):
			writer[i].writerow(["Id", "Class"] + sizes[i])
			writer[i + NUMFUN].writerow(["Id", "Class"] + sizes[i])

		# write all rows for training data
		for datafile in tqdm(os.listdir(DIR)):
			# get id and class
			id_str, clazz = datafile.split('.')[:2]
			clazz = util.malware_classes.index(clazz)
			# write to each file
			for i in range(NUMFUN):
				row = [id_str] + [clazz] + funs[i](datafile, DIR)
				writer[i].writerow(row)

		# write all rows for test data
		for datafile in tqdm(os.listdir(DIRR)):
			# get id and class
			id_str, clazz = datafile.split('.')[:2]
			clazz = -1
			# write to each file
			for i in range(NUMFUN):
				row = [id_str] + [clazz] + funs[i](datafile, DIRR)
				writer[i + NUMFUN].writerow(row)

def main ():
	write_to_csv()

if __name__ == "__main__":
    main()
#########################################################
#         baseline models and model tunning             #
#########################################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
## 1. Read in raw data
# use panda to read in data
df_train_0 = pd.read_csv("perSysCountsTrain.csv")
df_test_0 = pd.read_csv("perSysCountsTest.csv")
# view the train data
#df_train_0.head()
## 2. Clean data
# 2.1 combine train and test for easier feature engineering
# (1) delete 'Id' column
df_test = df_test_0.drop(['Id'], axis=1)
df_train = df_train_0.drop(['Id'], axis=1)
# Specify x and y
# 3.1 specify Class values as y
Y_train = df_train.Class.values

# 3.2 specify x values
X_train = df_train.drop(['Class'], axis=1).values
X_test = df_test.drop(['Class'], axis=1).values

# 3.3 view data structure
print("Train features:", X_train.shape)
print("Train Class:", Y_train.shape)
print("Test features:", X_test.shape)
# 4. split the training and test sets
x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.20)
from sklearn.linear_model import LogisticRegressionCV
# 1.1 Weighted Logistic Regression Classifier - one_vs_rest
# fit the model
wlogi_ovr = LogisticRegressionCV(class_weight='balanced', multi_class= 'ovr')
wlogi_ovr.fit(x_train, y_train)
# predict on train and test
wlogi_ovr_train = wlogi_ovr.predict(x_train)
wlogi_ovr_test = wlogi_ovr.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, wlogi_ovr_train))
print("accuracy for test set is: ", accuracy_score(y_test, wlogi_ovr_test))
# 1.2 Weighted Logistic Regression Classifier -multinomial
# fit the model
wlogi_multi = LogisticRegressionCV(class_weight='balanced', multi_class= 'multinomial')
wlogi_multi.fit(x_train, y_train)
# predict on train and test
wlogi_multi_train = wlogi_multi.predict(x_train)
wlogi_multi_test = wlogi_multi.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, wlogi_multi_train))
print("accuracy for test set is: ", accuracy_score(y_test, wlogi_multi_test))
# 1.3 Unweighted Logistic Regression Classifier - one_vs_rest
# fit the model
logi_ovr = LogisticRegressionCV(multi_class= 'ovr')
logi_ovr.fit(x_train, y_train)
# predict on train and test
logi_ovr_train = logi_ovr.predict(x_train)
logi_ovr_test = logi_ovr.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, logi_ovr_train))
print("accuracy for test set is: ", accuracy_score(y_test, logi_ovr_test))
# 1.4 Unweighted Logistic Regression Classifier -multinomial
# fit the model
logi_multi = LogisticRegressionCV(multi_class= 'multinomial')
logi_multi.fit(x_train, y_train)
# predict on train and test
logi_multi_train = logi_multi.predict(x_train)
logi_multi_test = logi_multi.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, logi_multi_train))
print("accuracy for test set is: ", accuracy_score(y_test, logi_multi_test))
# 3. Tree-based classifier
from sklearn.ensemble import ExtraTreesClassifier
# 3.1 Extra Trees Classifier
# fit the model
ET = ExtraTreesClassifier(n_estimators=30)
ET.fit(x_train, y_train)
# predict on train and test
ET_train = ET.predict(x_train)
ET_test = ET.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, ET_train))
print("accuracy for test set is: ", accuracy_score(y_test, ET_test))
# 3.2 Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
# fit the model
RF = RandomForestClassifier(n_estimators=30)
RF.fit(x_train, y_train)
# predict on train and test
RF_train = RF.predict(x_train)
RF_test = RF.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, RF_train))
print("accuracy for test set is: ", accuracy_score(y_test, RF_test))
# 4. SVM
from sklearn import svm
# 4.1 unweighted
# fit the model
svm = svm.SVC()
svm.fit(x_train, y_train)
# predict on train and test
svm_train = svm.predict(x_train)
svm_test = svm.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, svm_train))
print("accuracy for test set is: ", accuracy_score(y_test, svm_test))
# 4.2 weighted svm
# fit the model
wsvm = svm.SVC(class_weight='balanced')
wsvm.fit(x_train, y_train)
# predict on train and test
wsvm_train = wsvm.predict(x_train)
wsvm_test = wsvm.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, wsvm_train))
print("accuracy for test set is: ", accuracy_score(y_test, wsvm_test))
# 5. BNB
from sklearn.naive_bayes import BernoulliNB
BNB = BernoulliNB()
BNB.fit(x_train, y_train)
# predict on train and test
BNB_train = BNB.predict(x_train)
BNB_test = BNB.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, BNB_train))
print("accuracy for test set is: ", accuracy_score(y_test, BNB_test))
from sklearn.neural_network import MLPClassifier
NN = MLPClassifier(random_state=119)
NN.fit(x_train, y_train)
# predict on train and test
NN_train = NN.predict(x_train)
NN_test = NN.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, NN_train))
print("accuracy for test set is: ", accuracy_score(y_test, NN_test))
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
# predict on train and test
knn_train = knn.predict(x_train)
knn_test = knn.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, knn_train))
print("accuracy for test set is: ", accuracy_score(y_test, knn_test))
from xgboost.sklearn import XGBClassifier
xgb = XGBClassifier()
xgb_fit = xgb.fit(x_train,y_train)
# predict on train and test
xgb_train = xgb_fit.predict(x_train)
xgb_test = xgb_fit.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, xgb_train))
print("accuracy for test set is: ", accuracy_score(y_test, xgb_test))
test_ids = []
for ID in df_test_0['Id'].values:
    test_ids.append(ID)
def write_predictions(predictions, ids, outfile):
    """
    assumes len(predictions) == len(ids), and that predictions[i] is the
    index of the predicted class with the malware_classes list above for
    the executable corresponding to ids[i].
    outfile will be overwritten
    """
    with open(outfile,"w+") as f:
        # write header
        f.write("Id,Prediction\n")
        for i, history_id in enumerate(ids):
            f.write("%s,%d\n" % (history_id, predictions[i]))
# prediction from ET: 0.81474
ET_Test = ET.predict(X_test)
write_predictions(ET_Test,test_ids,"perSysCountsET.csv")
# prediction from NN: 0.75737
NN_Test = NN.predict(X_test)
write_predictions(NN_Test,test_ids,"perSysCountsNN.csv")
# prediction from XGB_final: 0.79842
xgb_Test = xgb.predict(X_test)
write_predictions(xgb_Test,test_ids,"xgb.csv")


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
# tunning the model use 5-cv
parameters = {"max_depth": [10,20,50,60,80,100]
                        #,"min_samples_split" :[2,10,50,500,1000]
                        ,"n_estimators" : [30,50,70]}
                        #,"min_samples_leaf": [2,10,50,500,1000]
                        #,"max_features": ('auto','sqrt','log2')
ET_regr = ExtraTreesClassifier()
model = GridSearchCV(ET_regr,parameters, cv = 10)
fit = model.fit(x_train,y_train)
learned_parameters = fit.best_params_
# Rerun model on fitted parameters
ET = ExtraTreesClassifier(max_depth = learned_parameters["max_depth"])
                            #,max_features = 'sqrt'
                            #,min_samples_leaf = learned_parameters['min_samples_leaf']
                            #,min_samples_split = learned_parameters['min_samples_split']
                            #,n_estimators = learned_parameters['n_estimators']
ET_fit = ET.fit(x_train,y_train)
print("The best max_depth is: ", learned_parameters["max_depth"])
print("The best n_estimators is: ", learned_parameters["n_estimators"])

from sklearn.neural_network import MLPClassifier
# tunning the model use 5-cv
parameters = {#"learning_rate_init": [0.001, 0.0001, 0.00001, 0.000001, 0.0000001]
                        #,"hidden_layer_sizes" :[(10,),(50,),(100,),(200,)]
                        #,"alpha" : [1,0.01,0.0001]
                        "activation": ('identity', 'logistic', 'tanh', 'relu')}
NN_regr = MLPClassifier()
model = GridSearchCV(NN_regr,parameters, cv = 5)
fit = model.fit(x_train,y_train)
learned_parameters = fit.best_params_

# Rerun model on fitted parameters
NN = MLPClassifier(#learning_rate_init = learned_parameters["learning_rate_init"])
                            #,hidden_layer_sizes = learned_parameters["hidden_layer_sizes"]
                            #,alpha = learned_parameters['alpha']
                            activation = learned_parameters['activation'])
NN_fit = NN.fit(x_train,y_train)
#print("The best learning_rate is: ", learned_parameters["learning_rate_init"])
#print("The best hidden_layer_sizes is: ", learned_parameters["hidden_layer_sizes"])
print("The best activation is: ", learned_parameters["activation"])

from xgboost.sklearn import XGBClassifier
# tunning the model use 5-cv
parameters = {#"max_depth": [1,2,3,4]
                        #,"learning_rate" :[0.1, 0.01, 0.001, 0.0001]
                        #,"n_estimators" : [10, 100, 200, 300]
                        "booster": ('gbtree', 'gblinear', 'dart')}
xgb_regr = XGBClassifier()
model = GridSearchCV(xgb_regr,parameters, cv = 7)
fit = model.fit(x_train,y_train)
learned_parameters = fit.best_params_

# Rerun model on fitted parameters
xgb = XGBClassifier(#max_depth = learned_parameters["max_depth"])
                            #,learning_rate = learned_parameters["learning_rate"]
                            #,n_estimators = learned_parameters['n_estimators']
                            booster = learned_parameters['booster'])
xgb_fit = xgb.fit(x_train,y_train)
#print("The best max_depth is: ", learned_parameters["max_depth"])
#print("The best learning_rate is: ", learned_parameters["learning_rate"])
#print("The best n_estimators is: ", learned_parameters["n_estimators"])
print("The best booster is: ", learned_parameters["booster"])
# predict on train and test
xgb_train = xgb_fit.predict(x_train)
xgb_test = xgb_fit.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, xgb_train))
print("accuracy for test set is: ", accuracy_score(y_test, xgb_test))
############################################################################
#                 feature selection and meta-classifier                    #
############################################################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from xgboost.sklearn import XGBClassifier
import seaborn as sb

# load in variable space
X_train = np.loadtxt("X_train_4.csv",delimiter=",")
X_test = np.loadtxt("X_test_4.csv",delimiter=",")
Y_train = np.loadtxt("Y_train_4.csv",delimiter=",")

# split the training and test sets
x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.20)

print("Before tunning the dimension of x_train is: ", x_train.shape)
print("Before tunning the dimension of x_test is: ", x_test.shape)

ET = ExtraTreesClassifier()
ET_fit = ET.fit(x_train,y_train)
# predict on train and test
ET_train = ET_fit.predict(x_train)
ET_test = ET_fit.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, ET_train))
print("accuracy for test set is: ", accuracy_score(y_test, ET_test))
ET_imp = ET.feature_importances_
i1 = (ET_imp == 0)
x_train_1 = x_train[:,i1 == False]
x_test_1 = x_test[:,i1 == False]
# view data structure after drop
print("Train features after 1 drop:", x_train_1.shape)
print("Test features after 1 drop:", x_test_1.shape)

xgb = XGBClassifier()
xgb_fit = xgb.fit(x_train,y_train)
# predict on train and test
xgb_train = xgb_fit.predict(x_train)
xgb_test = xgb_fit.predict(x_test)
print("accuracy for training set is: ", accuracy_score(y_train, xgb_train))
print("accuracy for test set is: ", accuracy_score(y_test, xgb_test))

xgb_imp = xgb.feature_importances_
i2 = (xgb_imp == 0)
x_train_2 = x_train[:,i2 == False]
x_test_2 = x_test[:,i2 == False]
X_test_2 = X_test[:,i2 == False]   # for final prediction
X_train_2 = X_train[:,i2 == False] # for final prediction
# view data structure after drop
print("Train features after 1 drop:", x_train_2.shape)
print("Test features after 1 drop:", x_test_2.shape)
print("Test features after 1 drop:", X_test_2.shape) # for final prediction
print("Test features after 1 drop:", X_train_2.shape)# for final prediction

ET_r = ExtraTreesClassifier()
ET_r_fit = ET_r.fit(x_train_1,y_train)
# predict on train and test
ET_r_train = ET_r_fit.predict(x_train_1)
ET_r_test = ET_r_fit.predict(x_test_1)
print("accuracy for training set is: ", accuracy_score(y_train, ET_r_train))
print("accuracy for test set is: ", accuracy_score(y_test, ET_r_test))

ET_imp_r = ET_r.feature_importances_
plt.hist(ET_imp_r, bins ='auto')
plt.title("Histogram with 'feature_importances'")
plt.xscale('log')
plt.xlabel("Impotance")
plt.ylabel("Counts of Features")
plt.savefig('his_feature_importance.png')
plt.show()

plt.bar(range(len(ET_imp_r)), ET_imp_r, 2)
plt.yscale('symlog')
plt.show()

ET_x = ExtraTreesClassifier()
ET_x_fit = ET_x.fit(x_train_2,y_train)
# predict on train and test
ET_x_train = ET_x_fit.predict(x_train_2)
ET_x_test = ET_x_fit.predict(x_test_2)
print("accuracy for training set is: ", accuracy_score(y_train, ET_x_train))
print("accuracy for test set is: ", accuracy_score(y_test, ET_x_test))

ET_imp_x = ET_x.feature_importances_
plt.hist(ET_imp_x, bins ='auto')
plt.title("Histogram with 'feature_importances'")
plt.xscale('log')
plt.xlabel("Impotance")
plt.ylabel("Counts of Features")
plt.savefig('his_feature_importance.png')
plt.show()
plt.bar(range(len(ET_imp_x)), ET_imp_x, 2)
plt.yscale('symlog')
plt.show()
xgb_r = XGBClassifier()
xgb_r_fit = xgb_r.fit(x_train_2,y_train)
# predict on train and test
xgb_r_train = xgb_r_fit.predict(x_train_2)
xgb_r_test = xgb_r_fit.predict(x_test_2)
print("accuracy for training set is: ", accuracy_score(y_train, xgb_r_train))
print("accuracy for test set is: ", accuracy_score(y_test, xgb_r_test))
xgb_imp_r = xgb_r.feature_importances_
plt.hist(xgb_imp_r, bins ='auto')
plt.title("Histogram with 'feature_importances'")
plt.xscale('log')
plt.xlabel("Impotance")
plt.ylabel("Counts of Features")
plt.savefig('his_feature_importance.png')
plt.show()
plt.bar(range(len(xgb_imp_r)), xgb_imp_r, 2)
plt.yscale('symlog')
plt.show()
from sklearn.feature_selection import VarianceThreshold
# delete features that have same values in all class
sel = VarianceThreshold() # can specify: 'threshold=(.8 * (1 - .8))'
x = sel.fit_transform(x_train_2)
x.shape
# Calculate pearson correlation Coefficient
pd_train = pd.DataFrame(x_train_2)
pd_test = pd.DataFrame(x_test_2)
pd_Test = pd.DataFrame(X_test_2)   # for final prediction, or 'X_test'
pd_Train = pd.DataFrame(X_train_2) # for final prediction, or 'X_train'
pd_train_pear = pd_train.corr(method="pearson")
plt.figure(figsize=(30,30))
sb.set(font_scale=0.7)
sb.heatmap(abs(pd_train_pear),cmap="YlGn",annot=False)
plt.title("A Quick Look at the Correlations among Predictors",fontsize=20);
plt.savefig('heatmap_pearson.pdf')
plt.show()
pd_train_1 = pd_train.copy()
pd_test_1 = pd_test.copy()
pd_Test_1 = pd_Test.copy()   # for final prediction
pd_Train_1 = pd_Train.copy() # for final prediction
col_corr = set() # Set of all the names of deleted columns
for i in range(len(pd_train_pear.columns)):
    for j in range(i):
        if pd_train_pear.iloc[i, j] >= 0.98:
            colname = pd_train_pear.columns[i] # getting the name of column
            col_corr.add(colname)
            if colname in pd_train_1.columns:
                del pd_Test_1[colname]   # for final prediction
                del pd_Train_1[colname]  # for final prediction
                del pd_test_1[colname]
                del pd_train_1[colname] # deleting the column from the dataset
pd_test_1 = pd_test_1.values
pd_train_1 = pd_train_1.values
pd_Test_1 = pd_Test_1.values
pd_Train_1 = pd_Train_1.values
# save the features
np.savetxt("X_train_4r.csv", pd_Train_1, delimiter=",")
np.savetxt("X_test_4r.csv", pd_Test_1, delimiter=",")
np.savetxt("Y_train_4r.csv", Y_train, delimiter=",")
############################################################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from xgboost.sklearn import XGBClassifier
from collections import Counter
from itertools import groupby

# load in variable space
X_train = np.loadtxt("X_train_PCS_BS.csv",delimiter=",")
X_test = np.loadtxt("X_test_PCS_BS.csv",delimiter=",")
Y_train = np.loadtxt("Y_train_PCS_BS.csv",delimiter=",")

# 1.1 set the parameters:
N = 50          # total number of predictions
Etree = N*0.4   # number of ExtraTree
GBoost = N*0.6  # number of XGB

# 1.2 generate prediction space:
predictions_train = np.zeros((X_train.shape[0], N+1))  # for Train
predictions_test = np.zeros((X_test.shape[0], N+1))    # for Test

for i in range(N+1):
    # split training and test sets
    #x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.20)
    if i < Etree:
        # fit model
        ET = ExtraTreesClassifier().fit(X_train,Y_train)
        # store predictions
        predictions_test[:,i] = ET.predict(X_test)
        predictions_train[:,i] = ET.predict(X_train)
    else:
        # fit model
        XGB = XGBClassifier().fit(X_train,Y_train)
        # store predictions
        predictions_test[:,i] = XGB.predict(X_test)
        predictions_train[:,i] = XGB.predict(X_train)

def random_mode(l):
    # group most_common output by frequency
    freqs = groupby(Counter(l).most_common(), lambda x:x[1])

    # pick off the first group (highest frequency)
    ll = []
    for val,count in freqs.__next__()[1]:
        ll.append(val)
    np.random.shuffle(ll)
    return ll[0]

meta_Test = np.zeros(X_test.shape[0])
for r in range(X_test.shape[0]):
    meta_Test[r] = random_mode(predictions_test[r])
meta_Train = np.zeros(X_train.shape[0])
for r in range(X_train.shape[0]):
    meta_Train[r] = random_mode(predictions_train[r])

print("accuracy for training set is: ", accuracy_score(Y_train, meta_Train))
