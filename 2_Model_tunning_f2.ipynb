{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS181_P2 Model_tunning for feature No.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I  load in the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in variable space\n",
    "X_train = np.loadtxt(\"X_train.csv\",delimiter=\",\")\n",
    "X_test = np.loadtxt(\"X_test.csv\",delimiter=\",\")\n",
    "Y_train = np.loadtxt(\"Y_train.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3086, 107)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II Models Tunning\n",
    "### 1. ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is:  60\n",
      "The best n_estimators is:  70\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {\"max_depth\": [10,20,50,60,80,100]\n",
    "                        #,\"min_samples_split\" :[2,10,50,500,1000]\n",
    "                        ,\"n_estimators\" : [30,50,70]}\n",
    "                        #,\"min_samples_leaf\": [2,10,50,500,1000]\n",
    "                        #,\"max_features\": ('auto','sqrt','log2')\n",
    "ET_regr = ExtraTreesClassifier()\n",
    "model = GridSearchCV(ET_regr,parameters, cv = 10)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "# Rerun model on fitted parameters \n",
    "ET = ExtraTreesClassifier(max_depth = learned_parameters[\"max_depth\"])\n",
    "                            #,max_features = 'sqrt'\n",
    "                            #,min_samples_leaf = learned_parameters['min_samples_leaf']\n",
    "                            #,min_samples_split = learned_parameters['min_samples_split']\n",
    "                            #,n_estimators = learned_parameters['n_estimators']\n",
    "ET_fit = ET.fit(x_train,y_train)\n",
    "print(\"The best max_depth is: \", learned_parameters[\"max_depth\"])\n",
    "print(\"The best n_estimators is: \", learned_parameters[\"n_estimators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.991085899514\n",
      "accuracy for test set is:  0.886731391586\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "ET_train = ET_fit.predict(x_train)\n",
    "ET_test = ET_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, ET_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, ET_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like ExtraTrees can't be improved further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network\n",
    "- **learning_rate_init**: The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’\n",
    "- **alpha**: L2 penalty (regularization term) parameter.\n",
    "- **activation**: Activation function for the hidden layer.\n",
    "    - **identity**: no-op activation, useful to implement linear bottleneck, returns **f(x) = x**\n",
    "    - **logistic**: the logistic sigmoid function, returns **f(x) = 1 / (1 + exp(-x))**.\n",
    "    - **tanh**: the hyperbolic tan function, returns **f(x) = tanh(x)**.\n",
    "    - **relu**: the rectified linear unit function, returns **f(x) = max(0, x)**\n",
    "- **hidden_layer_sizes**: The ith element represents the number of neurons in the ith hidden layer. length = n_layers - 2, default (100,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cut down the computation time, I tunned those parameters one by one:\n",
    "#### 1. activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xihanzhang/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/Users/xihanzhang/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n",
      "/Users/xihanzhang/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best activation is:  logistic\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {#\"learning_rate_init\": [0.001, 0.0001, 0.00001, 0.000001, 0.0000001]\n",
    "                        #,\"hidden_layer_sizes\" :[(10,),(50,),(100,),(200,)]\n",
    "                        #,\"alpha\" : [1,0.01,0.0001]\n",
    "                        \"activation\": ('identity', 'logistic', 'tanh', 'relu')}\n",
    "NN_regr = MLPClassifier()\n",
    "model = GridSearchCV(NN_regr,parameters, cv = 5)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "NN = MLPClassifier(#learning_rate_init = learned_parameters[\"learning_rate_init\"])\n",
    "                            #,hidden_layer_sizes = learned_parameters[\"hidden_layer_sizes\"]\n",
    "                            #,alpha = learned_parameters['alpha']\n",
    "                            activation = learned_parameters['activation'])\n",
    "NN_fit = NN.fit(x_train,y_train)\n",
    "#print(\"The best learning_rate is: \", learned_parameters[\"learning_rate_init\"])\n",
    "#print(\"The best hidden_layer_sizes is: \", learned_parameters[\"hidden_layer_sizes\"])\n",
    "print(\"The best activation is: \", learned_parameters[\"activation\"])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.940437601297\n",
      "accuracy for test set is:  0.86569579288\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "NN_train = NN_fit.predict(x_train)\n",
    "NN_test = NN_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, NN_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, NN_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tried several times and get the result: **\"logistic\"** is the best **activation**.\n",
    "#### 2. hidden_layer_sizes & learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best learning_rate is:  0.01\n",
      "The best hidden_layer_sizes is:  (400, 300)\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {\"learning_rate_init\": [0.01, 0.001, 0.0001]\n",
    "                        ,\"hidden_layer_sizes\" :[(400,200),(400,300),(400,400)]}\n",
    "                        #,\"alpha\" : [1,0.01,0.0001]\n",
    "                        #,\"activation\": ('identity', 'logistic', 'tanh', 'relu')\n",
    "NN_regr = MLPClassifier()\n",
    "model = GridSearchCV(NN_regr,parameters, cv = 5)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "NN_2 = MLPClassifier(learning_rate_init = learned_parameters[\"learning_rate_init\"]\n",
    "                            ,hidden_layer_sizes = learned_parameters[\"hidden_layer_sizes\"]\n",
    "                            #,alpha = learned_parameters['alpha']\n",
    "                            ,activation = 'logistic')\n",
    "NN_2_fit = NN_2.fit(x_train,y_train)\n",
    "print(\"The best learning_rate is: \", learned_parameters[\"learning_rate_init\"])\n",
    "print(\"The best hidden_layer_sizes is: \", learned_parameters[\"hidden_layer_sizes\"])\n",
    "#print(\"The best activation is: \", learned_parameters[\"activation\"])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.926661264182\n",
      "accuracy for test set is:  0.867313915858\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "NN_2_train = NN_2_fit.predict(x_train)\n",
    "NN_2_test = NN_2_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, NN_2_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, NN_2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)\n",
    "    - \"learning_rate_init\": [0.001, 0.00001, 0.0000001]\n",
    "    - \"hidden_layer_sizes\" :[(50,),(100,),(200,)]\n",
    "    - The best learning_rate is:  0.001\n",
    "    - The best hidden_layer_sizes is:  (200,)\n",
    "    - accuracy for training set is:  0.955834683955\n",
    "    - accuracy for test set is:  0.873786407767\n",
    "- (2)\n",
    "    - \"learning_rate_init\": [0.1, 0.01, 0.001]\n",
    "    - \"hidden_layer_sizes\" :[(200,),(300,),(400,)]\n",
    "    - The best learning_rate is:  0.01\n",
    "    - The best hidden_layer_sizes is:  (400,)\n",
    "    - accuracy for training set is:  0.931928687196\n",
    "    - accuracy for test set is:  0.875404530744\n",
    "- (3)\n",
    "    - \"hidden_layer_sizes\" :[(400,2),(400,5),(400,10),(400,50)]\n",
    "    - learning_rate_init = 0.01\n",
    "    - The best hidden_layer_sizes is:  (400, 50)\n",
    "    - accuracy for training set is:  0.918962722853\n",
    "    - accuracy for test set is:  0.857605177994\n",
    "- (4)\n",
    "    - \"learning_rate_init\": [0.01, 0.001, 0.0001]\n",
    "    - \"hidden_layer_sizes\" :[(400,200), (400,300), (400,400)]\n",
    "    - The best learning_rate is:  0.01\n",
    "    - The best hidden_layer_sizes is:  (400, 300)\n",
    "    - accuracy for training set is:  0.926661264182\n",
    "    - accuracy for test set is:  0.867313915858\n",
    "\n",
    "**Thus, I would use the parameters from(4)**:\n",
    "- learning_rate_init = 0.01\n",
    "- hidden_layer_sizes = (400, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is:  1e-05\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {#\"learning_rate_init\": [0.01, 0.001, 0.0001]\n",
    "                        #,\"hidden_layer_sizes\" :[(400,50),(400,100),(400,200)]\n",
    "                        \"alpha\" : [0.001, 0.0001, 0.00001, 0.000001]}\n",
    "                        #,\"activation\": ('identity', 'logistic', 'tanh', 'relu')\n",
    "NN_regr = MLPClassifier()\n",
    "model = GridSearchCV(NN_regr,parameters, cv = 5)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "NN_3 = MLPClassifier(learning_rate_init = 0.01\n",
    "                            ,hidden_layer_sizes = (400, 300)\n",
    "                            ,alpha = learned_parameters['alpha']\n",
    "                            ,activation = 'logistic')\n",
    "NN_3_fit = NN_3.fit(x_train,y_train)\n",
    "#print(\"The best learning_rate is: \", learned_parameters[\"learning_rate_init\"])\n",
    "#print(\"The best hidden_layer_sizes is: \", learned_parameters[\"hidden_layer_sizes\"])\n",
    "#print(\"The best activation is: \", learned_parameters[\"activation\"])  \n",
    "print(\"The best alpha is: \", learned_parameters[\"alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.927876823339\n",
      "accuracy for test set is:  0.873786407767\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "NN_3_train = NN_3_fit.predict(x_train)\n",
    "NN_3_test = NN_3_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, NN_3_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, NN_3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)\n",
    "    - \"alpha\" : [1,0.01,0.0001]\n",
    "    - The best alpha is:  0.0001\n",
    "    - accuracy for training set is:  0.948136142626\n",
    "    - accuracy for test set is:  0.875404530744\n",
    "    \n",
    "- (2)\n",
    "    - \"alpha\" : [0.001, 0.0001, 0.00001, 0.000001]\n",
    "    - The best alpha is:  0.00001\n",
    "    - accuracy for training set is:  0.927876823339\n",
    "    - accuracy for test set is:  0.873786407767\n",
    "Then, the final parameters are:\n",
    "- learning_rate_init = 0.01\n",
    "- hidden_layer_sizes = (400, 300)\n",
    "- alpha = 0.00001\n",
    "- activation = 'logistic'\n",
    "#### the final NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = MLPClassifier(learning_rate_init = 0.01, \n",
    "                   hidden_layer_sizes = (400, 300), \n",
    "                   alpha = 0.00001, \n",
    "                   activation = 'logistic')\n",
    "NN.fit(x_train, y_train)\n",
    "# predict on train and test\n",
    "NN_train = NN.predict(x_train)\n",
    "NN_test = NN.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.940437601297\n",
      "accuracy for test set is:  0.86569579288\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "NN_train = NN_fit.predict(x_train)\n",
    "NN_test = NN_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, NN_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, NN_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably **Neuro Net can't be improved too much**, and not as promissing as ExtraTrees.\n",
    "Then, I would switch to **Oxgboost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best booster is:  gbtree\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {#\"max_depth\": [1,2,3,4]\n",
    "                        #,\"learning_rate\" :[0.1, 0.01, 0.001, 0.0001]\n",
    "                        #,\"n_estimators\" : [10, 100, 200, 300]\n",
    "                        \"booster\": ('gbtree', 'gblinear', 'dart')}\n",
    "xgb_regr = XGBClassifier()\n",
    "model = GridSearchCV(xgb_regr,parameters, cv = 7)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "xgb = XGBClassifier(#max_depth = learned_parameters[\"max_depth\"])\n",
    "                            #,learning_rate = learned_parameters[\"learning_rate\"]\n",
    "                            #,n_estimators = learned_parameters['n_estimators']\n",
    "                            booster = learned_parameters['booster'])\n",
    "xgb_fit = xgb.fit(x_train,y_train)\n",
    "#print(\"The best max_depth is: \", learned_parameters[\"max_depth\"])\n",
    "#print(\"The best learning_rate is: \", learned_parameters[\"learning_rate\"])\n",
    "#print(\"The best n_estimators is: \", learned_parameters[\"n_estimators\"])\n",
    "print(\"The best booster is: \", learned_parameters[\"booster\"])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.959886547812\n",
      "accuracy for test set is:  0.893203883495\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "xgb_train = xgb_fit.predict(x_train)\n",
    "xgb_test = xgb_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, xgb_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, fix the booster as \"gbtree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is:  4\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {\"max_depth\": [4,8,10,20]}\n",
    "                        #,\"learning_rate\" :[0.1, 0.01, 0.001, 0.0001]\n",
    "                        #,\"n_estimators\" : [10, 100, 200, 300]\n",
    "                        #,\"booster\": ('gbtree', 'gblinear', 'dart')}\n",
    "xgb_regr = XGBClassifier()\n",
    "model = GridSearchCV(xgb_regr,parameters, cv = 7)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "xgb = XGBClassifier(max_depth = learned_parameters[\"max_depth\"]\n",
    "                            #,learning_rate = learned_parameters[\"learning_rate\"]\n",
    "                            #,n_estimators = learned_parameters['n_estimators']\n",
    "                            ,booster = 'gbtree')\n",
    "xgb_fit = xgb.fit(x_train,y_train)\n",
    "print(\"The best max_depth is: \", learned_parameters[\"max_depth\"])\n",
    "#print(\"The best learning_rate is: \", learned_parameters[\"learning_rate\"])\n",
    "#print(\"The best n_estimators is: \", learned_parameters[\"n_estimators\"])\n",
    "#print(\"The best booster is: \", learned_parameters[\"booster\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.974878444084\n",
      "accuracy for test set is:  0.901294498382\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "xgb_train = xgb_fit.predict(x_train)\n",
    "xgb_test = xgb_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, xgb_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)\n",
    "    - \"max_depth\": [1,2,3,4]\n",
    "    - The best max_depth is: 4\n",
    "    - accuracy for training set is:  0.974878444084\n",
    "    - accuracy for test set is:  0.901294498382\n",
    "- (2)\n",
    "    - \"max_depth\": [4,8,10,20]\n",
    "    - The best max_depth is: 4\n",
    "    - accuracy for training set is:  0.974878444084\n",
    "    - accuracy for test set is:  0.901294498382\n",
    "\n",
    "Thus, I'd choose **max_depth = 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best learning_rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {#\"max_depth\": [1,2,3,4]}\n",
    "                        \"learning_rate\" :[0.1, 0.01, 0.001, 0.0001]}\n",
    "                        #,\"n_estimators\" : [10, 100, 200, 300]\n",
    "                        #,\"booster\": ('gbtree', 'gblinear', 'dart')}\n",
    "xgb_regr = XGBClassifier()\n",
    "model = GridSearchCV(xgb_regr,parameters, cv = 7)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "xgb = XGBClassifier(#max_depth = learned_parameters[\"max_depth\"]\n",
    "                            learning_rate = learned_parameters[\"learning_rate\"]\n",
    "                            #,n_estimators = learned_parameters['n_estimators']\n",
    "                            ,booster = 'gbtree')\n",
    "xgb_fit = xgb.fit(x_train,y_train)\n",
    "#print(\"The best max_depth is: \", learned_parameters[\"max_depth\"])\n",
    "print(\"The best learning_rate is: \", learned_parameters[\"learning_rate\"])\n",
    "#print(\"The best n_estimators is: \", learned_parameters[\"n_estimators\"])\n",
    "#print(\"The best booster is: \", learned_parameters[\"booster\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.959886547812\n",
      "accuracy for test set is:  0.893203883495\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "xgb_train = xgb_fit.predict(x_train)\n",
    "xgb_test = xgb_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, xgb_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)\n",
    "    - \"learning_rate\" :[0.1, 0.01, 0.001, 0.0001]\n",
    "    - The best learning_rate is:  0.1\n",
    "    - accuracy for training set is:  0.959886547812\n",
    "    - accuracy for test set is:  0.893203883495\n",
    "    \n",
    "Thus, I'd fix **learning_rate = 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n_estimators is:  150\n"
     ]
    }
   ],
   "source": [
    "# tunning the model use 5-cv\n",
    "parameters = {#\"max_depth\": [1,2,3,4]}\n",
    "                        #,\"learning_rate\" :[0.1, 0.01, 0.001, 0.0001]}\n",
    "                        \"n_estimators\" : [150, 200, 250]}\n",
    "                        #,\"booster\": ('gbtree', 'gblinear', 'dart')}\n",
    "xgb_regr = XGBClassifier()\n",
    "model = GridSearchCV(xgb_regr,parameters, cv = 5)\n",
    "fit = model.fit(x_train,y_train)\n",
    "learned_parameters = fit.best_params_ \n",
    "\n",
    "# Rerun model on fitted parameters \n",
    "xgb = XGBClassifier(#max_depth = learned_parameters[\"max_depth\"]\n",
    "                            #,learning_rate = learned_parameters[\"learning_rate\"]\n",
    "                            n_estimators = learned_parameters['n_estimators']\n",
    "                            ,booster = 'gbtree')\n",
    "xgb_fit = xgb.fit(x_train,y_train)\n",
    "#print(\"The best max_depth is: \", learned_parameters[\"max_depth\"])\n",
    "#print(\"The best learning_rate is: \", learned_parameters[\"learning_rate\"])\n",
    "print(\"The best n_estimators is: \", learned_parameters[\"n_estimators\"])\n",
    "#print(\"The best booster is: \", learned_parameters[\"booster\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.970826580227\n",
      "accuracy for test set is:  0.894822006472\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "xgb_train = xgb_fit.predict(x_train)\n",
    "xgb_test = xgb_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, xgb_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)\n",
    "    - \"n_estimators\" : [10, 100, 200, 300]\n",
    "    - The best n_estimators is:  200\n",
    "    - accuracy for training set is:  0.977714748784\n",
    "    - accuracy for test set is:  0.899676375405\n",
    "- (2)\n",
    "    - \"n_estimators\" : [150, 200, 250]\n",
    "    - The best n_estimators is:  150\n",
    "    - accuracy for training set is:  0.970826580227\n",
    "    - accuracy for test set is:  0.894822006472\n",
    "\n",
    "Thus, set the **n_estimators = 200**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model for XGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 4\n",
    "                    ,learning_rate = 0.1\n",
    "                    ,n_estimators = 200\n",
    "                    ,booster = 'gbtree')\n",
    "xgb_fit = xgb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training set is:  0.986628849271\n",
      "accuracy for test set is:  0.909385113269\n"
     ]
    }
   ],
   "source": [
    "# predict on train and test\n",
    "xgb_train = xgb_fit.predict(x_train)\n",
    "xgb_test = xgb_fit.predict(x_test)\n",
    "print(\"accuracy for training set is: \", accuracy_score(y_train, xgb_train))\n",
    "print(\"accuracy for test set is: \", accuracy_score(y_test, xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_0 = pd.read_csv(\"perSysCountsTrain.csv\")\n",
    "df_test_0 = pd.read_csv(\"perSysCountsTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "for ID in df_test_0['Id'].values:\n",
    "    test_ids.append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predictions(predictions, ids, outfile):\n",
    "    \"\"\"\n",
    "    assumes len(predictions) == len(ids), and that predictions[i] is the\n",
    "    index of the predicted class with the malware_classes list above for\n",
    "    the executable corresponding to ids[i].\n",
    "    outfile will be overwritten\n",
    "    \"\"\"\n",
    "    with open(outfile,\"w+\") as f:\n",
    "        # write header\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i, history_id in enumerate(ids):\n",
    "            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction from xgb_Test: 0.81053\n",
    "xgb_Test = xgb_fit.predict(X_test)\n",
    "write_predictions(xgb_Test,test_ids,\"xgb_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction from NN_Test: 0.75737\n",
    "NN_Test = NN_fit.predict(X_test)\n",
    "write_predictions(NN_Test,test_ids,\"NN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
