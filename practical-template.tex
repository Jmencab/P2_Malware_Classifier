\documentclass[11pt]{article}

\usepackage{common}
\usepackage{booktabs}
\title{Practical 2: Sample Assignment}
\author{ 70939479 \\ Jazz # \\ Xihan #\\
Team JXJ: Jmencab, Xihan, JazzName}
\begin{document}


\maketitle{}



\section{Technical Approach}

How did you tackle the problem? Credit will be given for:

The main issue in this classification practical was extracting features. It seemed to us that the randomized feature extraction made little sense in terms of being able to compare the effectiveness of different models on a given data set as it would not be a consistent comparison. The first set of features we extracted was simply the number of system calls made per file. This we then converted to binary and did a classification on the resultant bit vector. This provided promising results when used with a Bernoulli Naive Bayes, logistic regression, random forest, and neural net (multi-layer perceptron), Gaussian Process Classifier. In order to improve upon this, however, we knew that we would need to get more expressive features. 

The next feature that we tried was a per-file, per-system-call count data set. This proved to be do much better with all the previously mentioned models that we tried on the simpler data set. We also tried XGBoost on this data set. Our results here were in some cases not as good as with the ostensibly less expressive previous data set. As such, we decided that hyperparameter tuning was in order.

%Xihan: please talk about the tuning???%

This tuning ultimately put us over the edge in terms of beating the $79\%$ baseline set by the bigram data. Taking inspiration fr0m a Microsoft Kaggle competition on malware detection, we decided to extend our features to 1,2,3,4-grams and tried combining the resulting csv's containing those n-grams to the per-file, per-system call counts file and running predictions based off that.

Below is a list showing the models used and the sets of features generated:

  \begin{table}
    \centering
    \begin{tabular}{@{}lll@{}}
     \toprule
      &\multicolumn{1}{c}{Features Generated} \\
      \midrule
      & Total System Calls\\
      & Per-system-call Counts \\
      & Bigram \\
      & 3-gram  \\
      & 4-gram \\
      \bottomrule
    \end{tabular}
    \caption{Feature sets generated from input data.}
  \end{table}
  
    \begin{table}
    \centering
    \begin{tabular}{@{}lll@{}}
     \toprule
      &\multicolumn{1}{c}{Models Used} \\
      \midrule
      & \\
      & Per-system-call Counts \\
      & Bigram \\
      & 3-gram  \\
      & 4-gram \\
      \bottomrule
    \end{tabular}
    \caption{Feature sets generated from input data.}
  \end{table}

  \begin{itemize}
  \item Diving deeply into a method (rather than just trying
    off-the-shelf tools with default settings). This can mean 
    providing mathematical descriptions or pseudo-code.
  \item Making tuning and configuration decisions using thoughtful experimentation.  
    This can mean carefully describing features added or hyperparameters tuned.
  \item Exploring several methods. This can contrasting two approaches
    or perhaps going beyond those we discussed in class.
  \end{itemize}

  \noindent Thoughtfully iterating on approaches is key.
  If you used existing packages or referred to papers or blogs for ideas,
  you should cite these in your report. 

  \begin{table}
    \centering
    \begin{tabular}{@{}lll@{}}
%      \toprule
      &\multicolumn{2}{c}{Mention Features  } \\
      & Feature & Value Set\\
      \midrule
      & Mention Head & $\mcV$ \\
      & Mention First Word & $\mcV$ \\
      & Mention Last Word & $\mcV$ \\
      & Word Preceding Mention & $\mcV$ \\
      & Word Following Mention & $\mcV$\\
      & \# Words in Mention & $\{1, 2, \ldots \}$ \\
      & Mention Type & $\mathcal{T}$ \\
      \bottomrule
      
    \end{tabular}
    \caption{Feature lists are a good way of illustrating problem specific tuning.}
  \end{table}



\section{Results}
This section should report on the following questions: 

\begin{itemize}
\item Did you create and submit a set of
  predictions? 
  

\item  Did your methods give reasonable performance?  
\end{itemize}

\noindent You must have \textit{at least one plot or table}
that details the performances of different methods tried. 
Credit will be given for quantitatively reporting (with clearly
labeled and captioned figures and/or tables) on the performance of the
methods you tried compared to your baselines.



\begin{table}
\centering
\begin{tabular}{llr}
 \toprule
 Model &  & Acc. \\
 \midrule
 \textsc{Baseline 1} & & 0.45\\
 \textsc{Baseline 2} & & 2.59 \\
 \textsc{Model 1} & & 10.59  \\
 \textsc{Model 2} & &13.42 \\
 \textsc{Model 3} & & 7.49\\
 \bottomrule
\end{tabular}
\caption{\label{tab:results} Result tables can compactly illustrate absolute performance, but a plot may be more effective at illustrating a trend.}
\end{table}




\section{Discussion} 


End your report by discussing the thought process behind your
analysis. This section does not need to be as technical as the others 
but should summarize why you took the approach that your did. Credit will be given for:

  \begin{itemize}
  \item Explaining the your reasoning for why you seqentially chose to
    try the approaches you did (i.e. what was it about your initial
    approach that made you try the next change?).  
  \item Explaining the results.  Did the adaptations you tried improve
    the results?  Why or why not?  Did you do additional tests to
    determine if your reasoning was correct?  
  \end{itemize}
 

\end{document}


